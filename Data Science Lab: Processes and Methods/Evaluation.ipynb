{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79079545",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/dev_and_eval.csv\")\n",
    "eva = eva.loc[eva[\"set\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva.drop(columns=[\"ids\", \"flag\", \"set\", \"sentiment\", \"sentiment_lbl\"], inplace=True)\n",
    "eva.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fa9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/dev_mod.csv\")\n",
    "eva = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/eval_mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19535f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/true_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c7dd5",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def orientation_data(df):\n",
    "    user_sentiment_cnt = defaultdict(lambda: [0, 0])\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        user_sentiment_cnt[row[\"user\"]][0] += row[\"sentiment\"]\n",
    "        user_sentiment_cnt[row[\"user\"]][1] += 1\n",
    "\n",
    "    max_tweets = max(user_sentiment_cnt.values(), key=lambda t: t[1])[1]\n",
    "\n",
    "    return user_sentiment_cnt, max_tweets\n",
    "\n",
    "def orientation(data, u, max_tweets):\n",
    "    if u in data:\n",
    "        return data[u][0] / max_tweets - (data[u][1] - data[u][0]) / max_tweets\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from html import unescape\n",
    "\n",
    "tags = re.compile(\"@\\w\")\n",
    "hashtags = re.compile(\"#\\w\")\n",
    "urls = re.compile(\"(http|https)?:?\\/?\\/?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\")\n",
    "esclamation = re.compile(\"\\!\")\n",
    "question = re.compile(\"\\?\")\n",
    "happy = re.compile(r\"([\\:\\;]'?-?[\\)DPp])\")\n",
    "sad = re.compile(r\"D-?'?\\:|[\\:\\;]['-]?-?[(\\\\\\/cC]\")\n",
    "\n",
    "def text_features(row):\n",
    "    row[\"text\"] = unescape(row[\"text\"])  # unescape html entities\n",
    "\n",
    "    row[\"#upper\"] = sum(map(str.isupper, row[\"text\"]))\n",
    "    row[\"#words\"] = len(row[\"text\"].split(\" \"))\n",
    "    row[\"#punct\"] = sum([1 for c in row[\"text\"] if c in  string.punctuation])\n",
    "    row[\"#!\"] = len(esclamation.findall(row[\"text\"]))\n",
    "    row[\"#?\"] = len(question.findall(row[\"text\"]))\n",
    "\n",
    "    row[\"#user_tags\"] = len(tags.findall(row[\"text\"]))\n",
    "    row[\"#hashtags\"] = len(hashtags.findall(row[\"text\"]))\n",
    "    row[\"#urls\"] = len(urls.findall(row[\"text\"]))\n",
    "    row[\"#happy_emot\"] = len(happy.findall(row[\"text\"]))\n",
    "    row[\"text\"] = happy.sub(\"_HAPPY_EMOT_\", row[\"text\"])\n",
    "    row[\"#sad_emot\"] = len(sad.findall(row[\"text\"]))\n",
    "    row[\"text\"] = sad.sub(\"_SAD_EMOT_\", row[\"text\"])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva = eva.apply(lambda r: text_features(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c490b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva[\"text\"] = eva.apply(lambda r: r[\"text\"] + \" \" + r[\"user\"] + \" \" + r[\"date\"].split(\" \")[0], axis=1)\n",
    "\n",
    "eva.to_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/eval_mod.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c8dad",
   "metadata": {},
   "source": [
    "# Test our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0340a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sw = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'he', 'she', \"she's\", 'it', \"it's\", 'they', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
    "    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'other', 'some', 'such',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'will', 'just', 'now', 'd', 'll', 'm', 'o',\n",
    "    're','ve','y','ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d434bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def text_prep(t):\n",
    "    t = t.lower()\n",
    "    wb = tk.tokenize(t)\n",
    "    wb = [w for w in wb if len(w) < 20]\n",
    "    wb = [w for w in wb if w not in custom_sw]\n",
    "    wst = nltk.pos_tag(wb)\n",
    "    wst_new = []\n",
    "    for e in wst:\n",
    "        wst_new.append((e[0], get_wordnet_pos(e[1])))\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    wl = [lemma.lemmatize(w, pos=p) if p != None else lemma.lemmatize(w) for w,p in wst_new]\n",
    "    return \" \".join([w for w in wl])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf10c65",
   "metadata": {},
   "source": [
    "## RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53723f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, maxtw = orientation_data(dev)\n",
    "\n",
    "dev[\"orientation\"] = dev[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "eva[\"orientation\"] = eva[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "\n",
    "dnum = dev[[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "enum = eva[[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "dnum = scaler.fit_transform(dnum)\n",
    "enum = scaler.transform(enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "best_rfc = {'binary': True, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2),\n",
    "            'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}\n",
    "\n",
    "tfidf = TfidfVectorizer(**best_rfc)\n",
    "X = tfidf.fit_transform(dev[\"text\"])\n",
    "x = tfidf.transform(eva[\"text\"])\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=500, random_state=20)\n",
    "X = tsvd.fit_transform(X)\n",
    "x = tsvd.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411dd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([X, dnum])\n",
    "x = np.hstack([x, enum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rfc = RandomForestClassifier(**{'criterion': 'entropy', 'max_features': 'sqrt', 'n_estimators': 400,\n",
    "                              'n_jobs': -1, 'random_state': 20})\n",
    "rfc.fit(X, dev[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98caaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(true_labels, rfc.predict(x))) # tuning 0.85213, testing 0.85208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "importances = rfc.feature_importances_\n",
    "stds = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.Series(importances, index=[f\"P{i}\" for i in range(500)]+[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"])\n",
    "forest_importances.sort_values(ascending=False, inplace=True)\n",
    "forest_importances = forest_importances[:5]\n",
    "stds = stds[:5]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances in RFC\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "plt.savefig(\"final_importance.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207550cc",
   "metadata": {},
   "source": [
    "## LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, maxtw = orientation_data(dev)\n",
    "\n",
    "dev[\"orientation\"] = dev[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "eva[\"orientation\"] = eva[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "\n",
    "dnum = dev[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "enum = eva[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dnum = scaler.fit_transform(dnum)\n",
    "enum = scaler.transform(enum)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "dori = scaler.fit_transform(dev[\"orientation\"].values.reshape(-1, 1))\n",
    "eori = scaler.fit_transform(eva[\"orientation\"].values.reshape(-1, 1))\n",
    "\n",
    "dnum = np.hstack([dnum, dori.reshape((-1, 1))])\n",
    "enum = np.hstack([enum, eori.reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "best_lsvc = {'binary': False, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2),\n",
    "            'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}\n",
    "\n",
    "tfidf = TfidfVectorizer(**best_lsvc)\n",
    "X2 = tfidf.fit_transform(dev[\"text\"])\n",
    "x2 = tfidf.transform(eva[\"text\"])\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=500, random_state=20)\n",
    "X2 = tsvd.fit_transform(X2)\n",
    "x2 = tsvd.transform(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc55f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.hstack([X2, dnum])\n",
    "x2 = np.hstack([x2, enum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lsvc = LinearSVC(**{\"C\": 0.32, \"class_weight\": None, \"dual\": False, \"max_iter\": 5000, \"random_state\": 20})\n",
    "lsvc.fit(X2, dev[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(true_labels, lsvc.predict(x2)))  # tunin 0.85218, testing 0.85202"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
