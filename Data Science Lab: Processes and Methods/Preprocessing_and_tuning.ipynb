{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypTfGG1UzgAd",
    "outputId": "6f418827-25ba-487e-b402-bff7395cb342"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLQPspmlznx4"
   },
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/dev_and_eval.csv\")\n",
    "dev = dev.loc[dev[\"set\"]==\"dev\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyiAHK55h3Lt",
    "outputId": "bbb608f4-dd4d-441e-ce60-855b96de13c6"
   },
   "outputs": [],
   "source": [
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU1Y7M0W0alh"
   },
   "outputs": [],
   "source": [
    "dev.drop_duplicates(\"ids\", keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5pPFnD8h6X3",
    "outputId": "a56d6003-a156-43ae-c077-289cfa51f51b"
   },
   "outputs": [],
   "source": [
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_57zUv_iIfk"
   },
   "outputs": [],
   "source": [
    "dev.drop(columns=[\"ids\", \"flag\", \"set\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "XFk4qLw31fKB",
    "outputId": "b66d5d47-a257-40cc-9f2b-afd61c519f5d"
   },
   "outputs": [],
   "source": [
    "dev.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT6vHAxh2R6l"
   },
   "source": [
    "# Feature engineering\n",
    "\n",
    "## Orientation\n",
    "Extract the total and positive number of tweets of each user. Used to compute the feature _orientation_ we have defined as:\n",
    "$$o(user, max_{tweets}) = \\frac{n_{user, pos}}{max_{tweets}} - \\frac{n_{uesr, neg}}{max_{tweets}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWHiaWQR03LD"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def orientation_data(df):\n",
    "    user_sentiment_cnt = defaultdict(lambda: [0, 0])\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        user_sentiment_cnt[row[\"user\"]][0] += row[\"sentiment\"]\n",
    "        user_sentiment_cnt[row[\"user\"]][1] += 1\n",
    "\n",
    "    max_tweets = max(user_sentiment_cnt.values(), key=lambda t: t[1])[1]\n",
    "\n",
    "    return user_sentiment_cnt, max_tweets\n",
    "\n",
    "def orientation(data, u, max_tweets):\n",
    "    if u in data:\n",
    "        return data[u][0] / max_tweets - (data[u][1] - data[u][0]) / max_tweets\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD_ULMe22d-5"
   },
   "source": [
    "## Features about text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMnuN1CS2HG0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from html import unescape\n",
    "\n",
    "tags = re.compile(\"@\\w\")\n",
    "hashtags = re.compile(\"#\\w\")\n",
    "urls = re.compile(\"(http|https)?:?\\/?\\/?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\")\n",
    "esclamation = re.compile(\"\\!\")\n",
    "question = re.compile(\"\\?\")\n",
    "happy = re.compile(r\"([\\:\\;]'?-?[\\)DPp])\")\n",
    "sad = re.compile(r\"D-?'?\\:|[\\:\\;]['-]?-?[(\\\\\\/cC]\")\n",
    "\n",
    "def text_features(row):\n",
    "    row[\"text\"] = unescape(row[\"text\"])  # unescape html entities\n",
    "\n",
    "    row[\"#upper\"] = sum(map(str.isupper, row[\"text\"]))\n",
    "    row[\"#words\"] = len(row[\"text\"].split(\" \"))\n",
    "    row[\"#punct\"] = sum([1 for c in row[\"text\"] if c in  string.punctuation])\n",
    "    row[\"#!\"] = len(esclamation.findall(row[\"text\"]))\n",
    "    row[\"#?\"] = len(question.findall(row[\"text\"]))\n",
    "\n",
    "    row[\"#user_tags\"] = len(tags.findall(row[\"text\"]))\n",
    "    row[\"#hashtags\"] = len(hashtags.findall(row[\"text\"]))\n",
    "    row[\"#urls\"] = len(urls.findall(row[\"text\"]))\n",
    "    row[\"#happy_emot\"] = len(happy.findall(row[\"text\"]))\n",
    "    row[\"text\"] = happy.sub(\"_HAPPY_EMOT_\", row[\"text\"])\n",
    "    row[\"#sad_emot\"] = len(sad.findall(row[\"text\"]))\n",
    "    row[\"text\"] = sad.sub(\"_SAD_EMOT_\", row[\"text\"])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF8QPFyMitAM"
   },
   "outputs": [],
   "source": [
    "dev = dev.apply(lambda r: text_features(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev[\"text\"] = dev.apply(lambda r: r[\"text\"] + \" \" + r[\"user\"] + \" \" + r[\"date\"].split(\" \")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "X-x_zjWsl9r9",
    "outputId": "b69e312c-a3da-4b03-bb4a-f60dd527fbca"
   },
   "outputs": [],
   "source": [
    "dev.to_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/dev_mod.csv\")\n",
    "dev.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqMF6ZG24IOg"
   },
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLTh8Ec9I6mz"
   },
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DSL/dev_mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(dev, dev[\"sentiment\"], test_size=0.25, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lye4Ff23o90Q"
   },
   "outputs": [],
   "source": [
    "custom_sw = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'he', 'she', \"she's\", 'it', \"it's\", 'they', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
    "    'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "    'under', 'again', 'further', 'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'other', 'some', 'such',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'will', 'just', 'now', 'd', 'll', 'm', 'o',\n",
    "    're','ve','y','ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTKpKHFHm91w"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def text_prep(t):\n",
    "    t = t.lower()\n",
    "    wb = tk.tokenize(t)\n",
    "    wb = [w for w in wb if len(w) < 20]\n",
    "    wb = [w for w in wb if w not in custom_sw]\n",
    "    wst = nltk.pos_tag(wb)\n",
    "    wst_new = []\n",
    "    for e in wst:\n",
    "        wst_new.append((e[0], get_wordnet_pos(e[1])))\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    wl = [lemma.lemmatize(w, pos=p) if p != None else lemma.lemmatize(w) for w,p in wst_new]\n",
    "    return \" \".join([w for w in wl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhUPa_cIqxRt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "params = {\n",
    "    \"binary\": [True, False],\n",
    "    \"use_idf\": [True, False],\n",
    "    \"ngram_range\": [(1,2)],\n",
    "    \"lowercase\": [True],\n",
    "    \"preprocessor\": [text_prep],\n",
    "    \"tokenizer\": [tk.tokenize],\n",
    "    \"max_features\": [5000]\n",
    "}\n",
    "\n",
    "for param in ParameterGrid(params):\n",
    "    tfidf = TfidfVectorizer(**param)\n",
    "    X = tfidf.fit_transform(X_tr[\"text\"])\n",
    "    x = tfidf.transform(X_te[\"text\"])\n",
    "\n",
    "    tsvd = TruncatedSVD(n_components=500, random_state=20)\n",
    "    X = tsvd.fit_transform(X)\n",
    "    x = tsvd.transform(x)\n",
    "\n",
    "    rfc = RandomForestClassifier(random_state=20, n_jobs=8)\n",
    "    rfc.fit(X, y_tr)\n",
    "    print(param, f1_score(y_te, rfc.predict(x)))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    x = scaler.transform(x)\n",
    "\n",
    "    lsvc = LinearSVC(random_state=20)\n",
    "    lsvc.fit(X, y_tr)\n",
    "    print(param, f1_score(y_te, lsvc.predict(x)))\n",
    "    \n",
    "    del rfc, lsvc, scaler\n",
    "    \n",
    "# tfidf_rfc {'binary': True, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2), 'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}\n",
    "# tfidf_lsvc {'binary': False, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2), 'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9XPkuSML5zW"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, maxtw = orientation_data(X_tr)\n",
    "\n",
    "X_tr[\"orientation\"] = X_tr[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "X_te[\"orientation\"] = X_te[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "\n",
    "Xnum = X_tr[[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\", \"#!\", \"#?\", \"#user_tags\",\n",
    "               \"#hashtags\", \"#urls\", \"#happy_emot\", \"#sad_emot\"]]\n",
    "xnum = X_te[[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\", \"#!\", \"#?\", \"#user_tags\",\n",
    "               \"#hashtags\", \"#urls\", \"#happy_emot\", \"#sad_emot\"]]\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "Xnum = scaler.fit_transform(Xnum)\n",
    "xnum = scaler.transform(xnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=20, n_jobs=-1)\n",
    "\n",
    "rfc.fit(Xnum, y_tr)\n",
    "\n",
    "importances = rfc.feature_importances_\n",
    "stds = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.Series(importances, index=[\"orientation\", \"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\", \"#!\", \"#?\", \"#user_tags\",\n",
    "               \"#hashtags\", \"#urls\", \"#happy_emot\", \"#sad_emot\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=stds, ax=ax)\n",
    "ax.set_title(\"Feature importances in RFC\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "plt.savefig(\"importance.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnum = X_tr[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "xnum = X_te[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "Xnum = scaler.fit_transform(Xnum)\n",
    "xnum = scaler.transform(xnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "best_rfc = {'binary': True, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2),\n",
    "            'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}\n",
    "\n",
    "tfidf = TfidfVectorizer(**best_rfc)\n",
    "X = tfidf.fit_transform(X_tr[\"text\"])\n",
    "x = tfidf.transform(X_te[\"text\"])\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=500, random_state=20)\n",
    "X = tsvd.fit_transform(X)\n",
    "x = tsvd.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([X, Xnum])\n",
    "x = np.hstack([x, xnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [200, 300, 400],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"random_state\": [20],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "\n",
    "for param in ParameterGrid(params):\n",
    "    rfc = RandomForestClassifier(**param)\n",
    "    rfc.fit(X, y_tr)\n",
    "    print(param, f1_score(y_te, rfc.predict(x)))\n",
    "    del rfc\n",
    "    \n",
    "# {'criterion': 'entropy', 'max_features': 'sqrt', 'n_estimators': 400, 'n_jobs': -1, 'random_state': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data, maxtw = orientation_data(X_tr)\n",
    "\n",
    "X_tr[\"orientation\"] = X_tr[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "X_te[\"orientation\"] = X_te[\"user\"].apply(lambda u: orientation(data, u, maxtw))\n",
    "\n",
    "Xnum = X_tr[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "xnum = X_te[[\"weekday\", \"hour\", \"timestamp\", \"#upper\", \"#words\", \"#punct\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xnum = scaler.fit_transform(Xnum)\n",
    "xnum = scaler.transform(xnum)\n",
    "\n",
    "Xnum = np.hstack([X_tr[\"orientation\"].values.reshape(-1, 1), Xnum])\n",
    "xnum = np.hstack([X_te[\"orientation\"].values.reshape(-1, 1), xnum])\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "Xnum = scaler.fit_transform(Xnum)\n",
    "xnum = scaler.transform(xnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "best_lsvc = {'binary': False, 'lowercase': True, 'max_features': 5000, 'ngram_range': (1, 2),\n",
    "            'preprocessor': text_prep, 'tokenizer': tk.tokenize, 'use_idf': False}\n",
    "\n",
    "tfidf = TfidfVectorizer(**best_lsvc)\n",
    "X = tfidf.fit_transform(X_tr[\"text\"])\n",
    "x = tfidf.transform(X_te[\"text\"])\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=500, random_state=20)\n",
    "X = tsvd.fit_transform(X)\n",
    "x = tsvd.transform(x)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "X = np.hstack([X, Xnum])\n",
    "x = np.hstack([x, xnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "params = {\n",
    "    \"C\": [i/100 for i in range(10, 101, 2)],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"dual\": [False],\n",
    "    \"max_iter\": [5000],\n",
    "    \"random_state\": [20]\n",
    "}\n",
    "\n",
    "for param in ParameterGrid(params):\n",
    "    lsvc = LinearSVC(**param)\n",
    "    lsvc.fit(X, y_tr)\n",
    "    print(param, f1_score(y_te, lsvc.predict(x)))\n",
    "    del lsvc\n",
    "    \n",
    "# {\"C\": 0.32, \"class_weight\": None, \"dual\": False, \"max_iter\": 5000, \"random_state\": 20}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
